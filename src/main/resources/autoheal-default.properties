# AutoHeal Default Configuration Properties

# Cache Configuration
autoheal.cache.maximum-size=10000
autoheal.cache.expire-after-write=24h
autoheal.cache.expire-after-access=2h
autoheal.cache.record-stats=true

# AI Service Configuration
# Supported providers: OPENAI, GOOGLE_GEMINI, ANTHROPIC_CLAUDE, DEEPSEEK, GROK, LOCAL_MODEL, OLLAMA, MOCK
autoheal.ai.provider=GOOGLE_GEMINI

# Specify the AI model to use (RECOMMENDED to specify explicitly)
# OpenAI models: gpt-4o-mini (fast), gpt-4o (accurate)
# Gemini models: gemini-2.0-flash (fast), gemini-1.5-pro (accurate)
# Claude models: claude-3-5-sonnet-20241022, claude-3-opus
autoheal.ai.model=gemini-2.0-flash

# API Key - use environment variable syntax ${ENV_VAR} or direct value
autoheal.ai.api-key=${GEMINI_API_KEY}

# Timeout for AI requests
# Recommended: 30s for Gemini, 60s for OpenAI (OpenAI is slower: 5-15s vs Gemini's 2-4s)
autoheal.ai.timeout=30s

# Maximum retry attempts for AI healing
# Recommended: 3 for Gemini, 5 for OpenAI (to handle occasional rate limiting)
autoheal.ai.max-retries=3

# Enable visual analysis (screenshot-based healing)
autoheal.ai.visual-analysis-enabled=true

# ==============================
# PROVIDER-SPECIFIC CONFIGURATIONS
# ==============================

# --- Google Gemini (Default - Fast & Free) ---
# autoheal.ai.provider=GOOGLE_GEMINI
# autoheal.ai.model=gemini-2.0-flash
# autoheal.ai.api-key=${GEMINI_API_KEY}
# autoheal.ai.timeout=30s
# autoheal.ai.max-retries=3

# --- OpenAI (Slower but Accurate) ---
# autoheal.ai.provider=OPENAI
# autoheal.ai.model=gpt-4o-mini
# autoheal.ai.api-key=${OPENAI_API_KEY}
# autoheal.ai.timeout=60s              # OpenAI is slower: 5-15s per request
# autoheal.ai.max-retries=5            # More retries for rate limiting

# --- Anthropic Claude ---
# autoheal.ai.provider=ANTHROPIC_CLAUDE
# autoheal.ai.model=claude-3-5-sonnet-20241022
# autoheal.ai.api-key=${ANTHROPIC_API_KEY}
# autoheal.ai.timeout=45s
# autoheal.ai.max-retries=3

# --- DeepSeek ---
# autoheal.ai.provider=DEEPSEEK
# autoheal.ai.model=deepseek-chat
# autoheal.ai.api-key=${DEEPSEEK_API_KEY}
# autoheal.ai.timeout=30s
# autoheal.ai.max-retries=3

# --- Grok ---
# autoheal.ai.provider=GROK
# autoheal.ai.model=grok-beta
# autoheal.ai.api-key=${GROK_API_KEY}
# autoheal.ai.timeout=30s
# autoheal.ai.max-retries=3

# ==============================
# LOCAL MODEL CONFIGURATION (OPTIONAL)
# ==============================
# Uncomment and configure these settings when using LOCAL_MODEL provider
# For setup instructions, see: https://github.com/SanjayPG/autoheal-locator#-local-model-setup
#
# autoheal.ai.provider=LOCAL_MODEL
# autoheal.ai.api-key=not-required                         # API key not needed for local models
# autoheal.ai.base-url=http://localhost:11434/v1           # Your local model endpoint (Ollama, LM Studio, vLLM, etc.)
# autoheal.ai.model=llama3.2:3b                            # Your local model name (e.g., llama3.2:3b, mistral, codellama)
# autoheal.ai.timeout=60s                                  # Longer timeout for local models (may be slower than cloud)
# autoheal.ai.max-retries=2                                # Fewer retries for local models
# autoheal.ai.visual-analysis-enabled=false                # Most local models don't support vision (set to true if your model supports it)
#
# Common local model endpoints:
# - Ollama: http://localhost:11434/v1
# - LM Studio: http://localhost:1234/v1
# - vLLM: http://localhost:8000/v1
# - LocalAI: http://localhost:8080/v1

# AI Token Configuration
autoheal.ai.max-tokens-dom=500
autoheal.ai.max-tokens-visual=1000
autoheal.ai.temperature-dom=0.1
autoheal.ai.temperature-visual=0.0

# Performance Configuration
autoheal.performance.thread-pool-size=8
autoheal.performance.element-timeout=10s
autoheal.performance.enable-metrics=true
autoheal.performance.max-concurrent-requests=50

# Resilience Configuration
autoheal.resilience.circuit-breaker-failure-threshold=5
autoheal.resilience.circuit-breaker-timeout=5m
autoheal.resilience.retry-max-attempts=3
autoheal.resilience.retry-delay=1s

# Logging Configuration
logging.level.com.autoheal=DEBUG
logging.level.com.autoheal.impl.ai=DEBUG
logging.level.com.autoheal.AutoHealLocator=DEBUG
logging.level.com.autoheal.impl.cache=DEBUG