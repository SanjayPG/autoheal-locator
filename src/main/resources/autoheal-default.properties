# AutoHeal Default Configuration Properties

# Cache Configuration
autoheal.cache.maximum-size=10000
autoheal.cache.expire-after-write=24h
autoheal.cache.expire-after-access=2h
autoheal.cache.record-stats=true

# AI Service Configuration
# Supported providers: OPENAI, GOOGLE_GEMINI, ANTHROPIC_CLAUDE, DEEPSEEK, GROK, LOCAL_MODEL, OLLAMA, MOCK
autoheal.ai.provider=GOOGLE_GEMINI
# Specify the AI model to use (leave blank to use provider default)
# Common models: gpt-4o-mini, gpt-4o, gemini-2.0-flash, gemini-1.5-pro, claude-3-5-sonnet-20241022, claude-3-opus
autoheal.ai.model=gemini-2.0-flash
autoheal.ai.api-key=${GEMINI_API_KEY}
autoheal.ai.timeout=30s
autoheal.ai.max-retries=3
autoheal.ai.visual-analysis-enabled=true

# Provider-specific API Keys (environment variables)
# OpenAI: ${OPENAI_API_KEY}
# Google Gemini: ${GEMINI_API_KEY}
# Anthropic Claude: ${ANTHROPIC_API_KEY}
# DeepSeek: ${DEEPSEEK_API_KEY}
# Grok: ${GROK_API_KEY}

# ==============================
# LOCAL MODEL CONFIGURATION (OPTIONAL)
# ==============================
# Uncomment and configure these settings when using LOCAL_MODEL provider
# For setup instructions, see: https://github.com/SanjayPG/autoheal-locator#-local-model-setup
#
# autoheal.ai.provider=LOCAL_MODEL
# autoheal.ai.api-key=not-required                         # API key not needed for local models
# autoheal.ai.base-url=http://localhost:11434/v1           # Your local model endpoint (Ollama, LM Studio, vLLM, etc.)
# autoheal.ai.model=llama3.2:3b                            # Your local model name (e.g., llama3.2:3b, mistral, codellama)
# autoheal.ai.timeout=60s                                  # Longer timeout for local models (may be slower than cloud)
# autoheal.ai.max-retries=2                                # Fewer retries for local models
# autoheal.ai.visual-analysis-enabled=false                # Most local models don't support vision (set to true if your model supports it)
#
# Common local model endpoints:
# - Ollama: http://localhost:11434/v1
# - LM Studio: http://localhost:1234/v1
# - vLLM: http://localhost:8000/v1
# - LocalAI: http://localhost:8080/v1

# AI Token Configuration
autoheal.ai.max-tokens-dom=500
autoheal.ai.max-tokens-visual=1000
autoheal.ai.temperature-dom=0.1
autoheal.ai.temperature-visual=0.0

# Performance Configuration
autoheal.performance.thread-pool-size=8
autoheal.performance.element-timeout=10s
autoheal.performance.enable-metrics=true
autoheal.performance.max-concurrent-requests=50

# Resilience Configuration
autoheal.resilience.circuit-breaker-failure-threshold=5
autoheal.resilience.circuit-breaker-timeout=5m
autoheal.resilience.retry-max-attempts=3
autoheal.resilience.retry-delay=1s

# Logging Configuration
logging.level.com.autoheal=DEBUG
logging.level.com.autoheal.impl.ai=DEBUG
logging.level.com.autoheal.AutoHealLocator=DEBUG
logging.level.com.autoheal.impl.cache=DEBUG